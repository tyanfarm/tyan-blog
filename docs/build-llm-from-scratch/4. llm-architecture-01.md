---
sidebar_position: 8
title: 4. LLM Architecture (Part 01)
---

## 4.1. Code LLM architecture

### GPT Backbone
- Các mô hình Ngôn ngữ lớn (LLM) như GPT (Generative Pretrained Transformer) là những kiến trúc mạng nơ-ron có kích thước khổng lồ. 

- Tuy nhiên, kiến trúc của mô hình này thực chất lại ít phức tạp hơn bạn tưởng, bởi nhiều thành phần bên trong được lặp đi lặp lại (chúng ta sẽ tìm hiểu sau).

- ![alt](./imgs/llm-01-overall.png)

- Phía trên là hình mô tả tổng quát về _1 mô hình GPT_. Bên cạnh các `embedding layers` & `inputs tokenization`, nó bao gồm _1 hoặc nhiều_ khối `Transformer` chứa module `masked multi-head attention` (đã triển khai ở chương trước). 

- Trọng tâm ở chương này sẽ triển khai _core_ của LLM, bao gồm các _khối transformer_, sau đó chúng ta sẽ _huấn luyện_ ở chương tiếp theo để tạo ra văn bản giống con người.

- Dưới đây là mô tả các bước để build 1 mô hình GPT hoàn chỉnh.
- ![alt](./imgs/llm-02-backbone.png)

- Bắt đầu với bước đầu tiên là xây dựng `GPT Backbone`, ta sẽ triển khai 1 class là `DummyGPTModel` - phiên bản đơn giản hóa của mô hình GPT, sử dụng module neural network của Pytorch _nn.Module_. Kiến trúc của mô hình được triển khai tại phần `1. GPT Backbone` trong [`11. Placeholder-GPT.ipynb`](https://github.com/tyanfarm/build-LLM-from-scratch-notebook/blob/main/11.%20Placeholder-GPT.ipynb). 

### Layer Normalization
- Việc huấn luyện `deep neural network` (mạng nơ-ron sâu) với nhiều tầng `hidden layer` dễ gặp vấn đề như _vanishing gradient (triệt tiêu đạo hàm)_ hay _exploding gradient (bùng nổ đạo hàm)_. Điều này có nghĩa mô hình sẽ gặp khó khăn trong việc tìm ra bộ tham số tối ưu để giảm _loss_.

- Phần này chúng ta sẽ triển khai `layer normalization`. Ý tưởng chính là điều chỉnh các giá trị `activations` của 1 _hidden layer_ sao cho chúng có _giá trị trung bình ([mean](../machine-learning/base-math/probability-statistics.md))_ bằng 0 và _phương sai ([variance](../machine-learning/base-math/probability-statistics.md))_ bằng 1 - việc điều chỉnh _mean_ & _variance_ này là của [Standardization](./bonus-section/normalization.md). Để ý thêm 1 điều là tại sao lại chuẩn hóa theo 2 giá trị này, thì bởi vì [Phân phối chuẩn](../machine-learning/base-math/probability-statistics.md) dựa trên 2 tham số này.