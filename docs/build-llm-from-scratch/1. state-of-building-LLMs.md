---
sidebar_position: 1
title: 1. State of building LLMs
---

## Introduction Pipelines
![alt](./imgs/pretrained&finetuning.png)

- BÆ°á»›c Ä‘áº§u tiÃªn Ä‘á»ƒ xÃ¢y dá»±ng 1 LLM lÃ  huáº¥n luyá»‡n trÃªn má»™t táº­p vÄƒn báº£n khá»•ng lá»“ (raw text). Táº­p nÃ y lÃ  vÄƒn báº£n thÃ´ng thÆ°á»ng, khÃ´ng cÃ³ thÃ´ng tin nhÃ£n.

- Giai Ä‘oáº¡n huáº¥n luyá»‡n Ä‘áº§u tiÃªn nÃ y cÃ²n Ä‘Æ°á»£c gá»i lÃ  `pretraining`, táº¡o ra má»™t mÃ´ hÃ¬nh LLM sÆ¡ cáº¥p (pretrained LLM), thÆ°á»ng Ä‘Æ°á»£c gá»i lÃ  `base model` hoáº·c `foundation model`. MÃ´ hÃ¬nh nÃ y cÃ³ kháº£ nÄƒng hoÃ n thÃ nh vÄƒn báº£n (`text completion`), tá»©c lÃ  hoÃ n táº¥t má»™t cÃ¢u dá»Ÿ dang do ngÆ°á»i dÃ¹ng nháº­p vÃ o. NÃ³ cÅ©ng cÃ³ kháº£ nÄƒng `few-shot learning` á»Ÿ má»©c háº¡n cháº¿, nghÄ©a lÃ  cÃ³ thá»ƒ há»c cÃ¡ch thá»±c hiá»‡n má»™t nhiá»‡m vá»¥ má»›i chá»‰ dá»±a vÃ o má»™t vÃ i vÃ­ dá»¥, thay vÃ¬ cáº§n má»™t lÆ°á»£ng dá»¯ liá»‡u huáº¥n luyá»‡n lá»›n.

- Sau khi cÃ³ má»™t mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c `pretrain` (tá»« viá»‡c huáº¥n luyá»‡n trÃªn táº­p dá»¯ liá»‡u vÄƒn báº£n lá»›n, trong Ä‘Ã³ LLM há»c cÃ¡ch dá»± Ä‘oÃ¡n tá»« tiáº¿p theo trong chuá»—i vÄƒn báº£n), chÃºng ta cÃ³ thá»ƒ huáº¥n luyá»‡n tiáº¿p trÃªn dá»¯ liá»‡u cÃ³ nhÃ£n, cÃ²n Ä‘Æ°á»£c gá»i lÃ  `finetuning`.

- CÃ³ 2 hÆ°á»›ng `finetuning` phá»• biáº¿n:
    + `Instruction-finetuning`: Chatbot, Translator, Q&Aâ€¦
    + `Classification tasks finetuning`: Spam detection, sentiment analysis, intent detection

## Original Transformers Architecture

### 1. Encoder & Decoder
![alt](./imgs/original-transformers.png)

- Kiáº¿n trÃºc `transformers` gá»“m 2 module lÃ  `Encoder` & `Decoder`.

#### Token -> Embedding
    + â€œTheâ€ â†’ [0.2, 0.7, -0.1, â€¦]
    + â€œcatâ€ â†’ [0.5, 0.1, 0.8, â€¦]
    + â€œsatâ€ â†’ [0.9, -0.3, 0.4, â€¦]

#### Encoder
- CÃ¡c embedding Ä‘Æ°á»£c Ä‘Æ°a qua nhiá»u layer `self-attention` + `feed-forward` Ä‘á»ƒ náº¯m báº¯t Ä‘áº·c trÆ°ng cá»§a cÃ¡c token xung quanh.
- Tá»« Ä‘Ã³, á»Ÿ output cá»§a Encoder, má»—i token cÃ³ 1 `contextual embedding` (embedding giÃ u ngá»¯ cáº£nh).

#### Decoder
- `Masked self-attention`: Decoder tá»± nhÃ¬n vÃ o cÃ¡c token Ä‘Ã£ sinh trÆ°á»›c Ä‘Ã³ Ä‘á»ƒ dá»± Ä‘oÃ¡n token káº¿ tiáº¿p.

- `Cross-attention`: Decoder nhÃ¬n vÃ o toÃ n bá»™ embedding cá»§a Encoder Ä‘á»ƒ quyáº¿t Ä‘á»‹nh thÃ´ng tin nÃ o cáº§n dÃ¹ng khi sinh token tiáº¿p theo.

- `Feed-forward (FNN)`: Ä‘Æ°a qua 1 máº¡ng MLP Ä‘á»ƒ há»c Ä‘Æ°á»£c Ä‘áº·c trÆ°ng phi tuyáº¿n.


#### Example
- ğŸ‹ï¸`TRAINING`:

    + Input: `"This is an example"`
    + Target: `"Das ist ein Beispiel"`

    + CÃ¡ch mÃ´ hÃ¬nh há»c:
        + 1. Encoder nháº­n `"This is an example"` $\rightarrow$ sinh embedding vectors.
        + 2. Decoder nháº­n `Target` nhÆ°ng `dá»‹ch lá»‡ch pháº£i (shifted right)`
            - Input cho Decoder: `"<BOS> Das ist ein"` (trong Ä‘Ã³ `<BOS> = Begin Of Sentence)`.
            - Output cáº§n dá»± Ä‘oÃ¡n: `"Das ist ein Beispiel <EOS>"`.
        + 3. Nhá» Ä‘Ã³, mÃ´ hÃ¬nh há»c Ä‘Æ°á»£c cÃ¡ch dá»± Ä‘oÃ¡n token tiáº¿p theo:
            - Khi input `<BOS>`, output pháº£i lÃ  `"Das"`.
            - Khi input `"Das"`, output pháº£i lÃ  `"ist"`.
            - Khi input `"Das ist ein"`, output pháº£i lÃ  `"Beispiel"`.
    - ÄÃ¢y cÃ²n gá»i lÃ  `teacher forcing`.

- ğŸ¤–`INFERENCE`:
    
    + 1. Khi Encoder nháº­n `"This is an example"`.
    + 2. Decoder khá»Ÿi Ä‘áº§u chá»‰ cÃ³ `<BOS>`. Sau Ä‘Ã³ dá»± Ä‘oÃ¡n tá»« Ä‘áº§u tiÃªn: `"Das"`.
    + 3. Láº¥y `"Das"` lÃ m input tiáº¿p theo cho Decoder. Dá»± Ä‘oÃ¡n tá»« thá»© 2: `"ist"`.
    + 4. Input cho Decoder lÃºc nÃ y: `"Das ist"`. Dá»± Ä‘oÃ¡n tá»« tiáº¿p: `"ein"`.
    + 5. Input `"Das ist ein"`, dá»± Ä‘oÃ¡n `"Beispiel"`.

    + MÃ´ hÃ¬nh cá»© tháº¿ `tá»± sinh â†’ náº¡p láº¡i â†’ sinh tiáº¿p` cho Ä‘áº¿n khi gáº·p token Ä‘áº·c biá»‡t `<EOS>` (End Of Sentence).

### 2. BERT & GPT
![alt](./imgs/bert&gpt.png)

- *BERT (Bidirectional Encoder Representations from Transformers)*:

    + ÄÆ°á»£c xÃ¢y dá»±ng tá»« kiáº¿n trÃºc Encoder cá»§a Transformer gá»‘c. BERT khÃ´ng dÃ¹ng Ä‘á»ƒ sinh vÄƒn báº£n mÃ  táº­p trung vÃ o `Masked Language Modeling (MLM)` - che giáº¥u má»™t sá»‘ tá»« trong cÃ¢u rá»“i yÃªu cáº§u mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n láº¡i.

- *GPT (Generative Pretrained Transformers)*:

    + NgÆ°á»£c láº¡i, GPT dá»±a trÃªn Decoder cá»§a Transformer, Ä‘Æ°á»£c sá»­ dá»¥ng cho tÃ¡c vá»¥ sinh vÄƒn báº£n: *dá»‹ch mÃ¡y, tÃ³m táº¯t, viáº¿t code, ...*

    + NgoÃ i *text completion*, cÃ¡c mÃ´ hÃ¬nh LLM giá»‘ng GPT cÃ³ thá»ƒ giáº£i quyáº¿t nhiá»u task khÃ¡c nhau mÃ  khÃ´ng cáº§n retraining hay finetuning. Trong vÃ i trÆ°á»ng há»£p, viá»‡c thiáº¿t láº­p output mong muá»‘n trong pháº§n input Ä‘Æ°á»£c gá»i lÃ  `few-shot`. NgoÃ i ra, GPT cÅ©ng cÃ³ kháº£ nÄƒng thá»±c hiá»‡n task ngay cáº£ khi khÃ´ng cáº§n vÃ­ dá»¥ cá»¥ thá»ƒ, cÃ²n gá»i lÃ  `zero-shot`.
    
![alt](./imgs/zeroshot&fewshot.jpg)

## Summary
- CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM) Ä‘Æ°á»£c huáº¥n luyá»‡n theo 2 bÆ°á»›c chÃ­nh. TrÆ°á»›c tiÃªn, chÃºng Ä‘Æ°á»£c pretrained trÃªn má»™t kho vÄƒn báº£n khá»•ng lá»“ khÃ´ng gáº¯n nhÃ£n báº±ng cÃ¡ch dá»± Ä‘oÃ¡n tá»« tiáº¿p theo trong má»™t cÃ¢u nhÆ° lÃ  "nhÃ£n". Sau Ä‘Ã³, chÃºng Ä‘Æ°á»£c finetuned trÃªn má»™t táº­p nhá» hÆ¡n, cÃ³ gáº¯n nhÃ£n Ä‘á»ƒ lÃ m theo instruction hoáº·c thá»±c hiá»‡n tÃ¡c vá»¥ classification.

- LLM dá»±a trÃªn kiáº¿n trÃºc Transformer. Key idea cá»§a Transformer lÃ  cÆ¡ cháº¿ `attention`, cho phÃ©p LLM cÃ³ thá»ƒ "nhÃ¬n" Ä‘Æ°á»£c toÃ n bá»™ chuá»—i Ä‘áº§u vÃ o trÆ°á»›c khi táº¡o ra tá»«ng tá»«.

- Kiáº¿n trÃºc Transformer bao gá»“m 1 bá»™ mÃ£ hÃ³a (Encoder) Ä‘á»ƒ phÃ¢n tÃ­ch vÄƒn báº£n & 1 bá»™ giáº£i mÃ£ (Decoder) Ä‘á»ƒ sinh vÄƒn báº£n.

- CÃ¡c LLM nhÆ° GPT-3 & ChatGPT chá»‰ triá»ƒn khai cÃ¡c module Decoder.

- Nhiá»‡m vá»¥ cá»§a quÃ¡ trÃ¬nh pretrained LLM lÃ  dá»± Ä‘oÃ¡n tá»« tiáº¿p theo, nhÆ°ng LLM láº¡i thá»ƒ hiá»‡n cÃ¡c tÃ­nh cháº¥t ná»•i lÃªn nhÆ° classification, translation hay summarize texts.